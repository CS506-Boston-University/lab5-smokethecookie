{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Generate two 1D Gaussians (2 points)\n",
    "\n",
    "Here, you will generate **1D Gaussian (normal) distributions** using NumPy, and then visualize them with histograms and density curves.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Generate two Gaussian-distributed datasets:\n",
    "   - The first centered at **0** with variance **1** (standard normal).\n",
    "   - The second centered at **5** with variance **1**.\n",
    "   - Use about **1000 samples** for each.\n",
    "\n",
    "This will help you see what it means for data to come from a *mixture of Gaussians*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 1D Gaussians\n",
    "# ----------------------------\n",
    "\n",
    "data1 = np.random.normal(loc=0.0, scale=1.0, size=1000)\n",
    "data2 = np.random.normal(loc=5.0, scale=1.0, size=1000)\n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"value\": np.concatenate([data1, data2]),\n",
    "    \"distribution\": [\"Gaussian 1\"] * len(data1) + [\"Gaussian 2\"] * len(data2)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(\n",
    "    data=df, \n",
    "    x=\"value\", \n",
    "    hue=\"distribution\",\n",
    "    fill=True,       \n",
    "    common_norm=False,\n",
    "    alpha=0.5, \n",
    "    linewidth=2,\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "\n",
    "plt.title(\"Two 1D Gaussian Distributions\", fontsize=14)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Generate two 2D Gaussians (2 points)\n",
    "\n",
    "Now let’s move from **1D** to **2D** Gaussians.  \n",
    "A 2D Gaussian distribution is defined by:  \n",
    "- A **mean vector** (e.g., `[0, 0]`)  \n",
    "- A **covariance matrix** (e.g., `[[1, 0], [0, 1]]`)  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Generate two 2D Gaussian-distributed datasets using `np.random.multivariate_normal`.\n",
    "   - First Gaussian: mean = `[0, 0]`, covariance = `[[1, 0.5], [0.5, 1]]`\n",
    "   - Second Gaussian: mean = `[3, 3]`, covariance = `[[1, -0.3], [-0.3, 1]]`\n",
    "   - Use about **500 samples** for each.\n",
    "\n",
    "This will help you understand how clusters look in **2D Gaussian space**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 2D Gaussians\n",
    "# ----------------------------\n",
    "\n",
    "mean1 = np.array([0.0, 0.0])\n",
    "cov1  = np.array([[1.0, 0.5],\n",
    "                  [0.5, 1.0]])\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, size=500)\n",
    "\n",
    "mean2 = np.array([3.0, 3.0])\n",
    "cov2  = np.array([[1.0, -0.3],\n",
    "                  [-0.3, 1.0]])\n",
    "data2 = np.random.multivariate_normal(mean2, cov2, size=500)\n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# 2D Density Surface\n",
    "# ----------------------------\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], alpha=0.5, color='blue', label=\"Gaussian 1\")\n",
    "plt.scatter(data2[:, 0], data2[:, 1], alpha=0.5, color='red', label=\"Gaussian 2\")\n",
    "\n",
    "sns.kdeplot(x=data1[:,0], y=data1[:,1], levels=10, color='blue', alpha=0.5)\n",
    "sns.kdeplot(x=data2[:,0], y=data2[:,1], levels=10, color='red', alpha=0.5)\n",
    "\n",
    "plt.title(\"Two 2D Gaussian Distributions (Contours)\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "\n",
    "# ----------------------------\n",
    "# 3D Density Surface\n",
    "# ----------------------------\n",
    "\n",
    "x, y = np.meshgrid(np.linspace(-4, 8, 100), np.linspace(-4, 8, 100))\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "rv1 = multivariate_normal(mean1, cov1)\n",
    "rv2 = multivariate_normal(mean2, cov2)\n",
    "\n",
    "z1 = rv1.pdf(pos)\n",
    "z2 = rv2.pdf(pos)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "views = [(30, 45), (0, 120), (20, 200)]  \n",
    "titles = [\"View 1\", \"View 2\", \"View 3\"]\n",
    "\n",
    "for i, (elev, azim) in enumerate(views, 1):\n",
    "    ax = fig.add_subplot(1, 3, i, projection=\"3d\")\n",
    "    ax.plot_surface(x, y, z1, cmap=\"Blues\", linewidth=0, alpha=0.9)\n",
    "    ax.plot_surface(x, y, z2, cmap=\"Reds\", linewidth=0, alpha=0.9)\n",
    "\n",
    "    ax.set_title(titles[i-1])\n",
    "    ax.set_xlabel(\"X1\")\n",
    "    ax.set_ylabel(\"X2\")\n",
    "    ax.set_zlabel(\"Y\")\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Generate three 2D Gaussians (3 points)\n",
    "\n",
    "Now let’s go one step further and generate **three 2D Gaussian distributions**.  \n",
    "A 2D Gaussian distribution is defined by:  \n",
    "- A **mean vector** (e.g., `[0, 0]`)  \n",
    "- A **covariance matrix** (e.g., `[[1, 0], [0, 1]]`)  \n",
    "\n",
    "**Your tasks:**  \n",
    "1. Generate three 2D Gaussian-distributed datasets using `np.random.multivariate_normal`.  \n",
    "   - First Gaussian: mean = `[0, 0]`, covariance = `[[1, 0.3], [0.3, 1]]`  \n",
    "   - Second Gaussian: mean = `[4, 4]`, covariance = `[[1, -0.4], [-0.4, 1]]`  \n",
    "   - Third Gaussian: mean = `[0, 5]`, covariance = `[[1, 0.2], [0.2, 1]]`  \n",
    "   - Use about **500 samples** for each.  \n",
    "\n",
    "\n",
    "This will give you an intuition for how **multiple Gaussian clusters overlap** in 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 3 2D Gaussians\n",
    "# ----------------------------\n",
    "mean1 = np.array([0.0, 0.0])\n",
    "cov1  = np.array([[1.0, 0.3],\n",
    "                  [0.3, 1.0]])\n",
    "\n",
    "mean2 = np.array([4.0, 4.0])\n",
    "cov2  = np.array([[1.0, -0.4],\n",
    "                  [-0.4, 1.0]])\n",
    "\n",
    "mean3 = np.array([0.0, 5.0])\n",
    "cov3  = np.array([[1.0, 0.2],\n",
    "                  [0.2, 1.0]])\n",
    "\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, size=500)\n",
    "data2 = np.random.multivariate_normal(mean2, cov2, size=500)\n",
    "data3 = np.random.multivariate_normal(mean3, cov3, size=500)\n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# --- Subplot 1: 2D scatter + KDE contours ---\n",
    "ax1 = fig.add_subplot(1, 4, 1)\n",
    "ax1.scatter(data1[:, 0], data1[:, 1], alpha=0.4, color=\"blue\", label=\"Gaussian 1\")\n",
    "ax1.scatter(data2[:, 0], data2[:, 1], alpha=0.4, color=\"red\", label=\"Gaussian 2\")\n",
    "ax1.scatter(data3[:, 0], data3[:, 1], alpha=0.4, color=\"green\", label=\"Gaussian 3\")\n",
    "\n",
    "sns.kdeplot(x=data1[:, 0], y=data1[:, 1], levels=8, color=\"blue\", alpha=0.6, ax=ax1)\n",
    "sns.kdeplot(x=data2[:, 0], y=data2[:, 1], levels=8, color=\"red\", alpha=0.6, ax=ax1)\n",
    "sns.kdeplot(x=data3[:, 0], y=data3[:, 1], levels=8, color=\"green\", alpha=0.6, ax=ax1)\n",
    "\n",
    "ax1.set_title(\"2D Gaussian Distributions\")\n",
    "ax1.set_xlabel(\"X1\")\n",
    "ax1.set_ylabel(\"X2\")\n",
    "ax1.legend()\n",
    "\n",
    "# --- Setup for 3D PDFs ---\n",
    "x, y = np.meshgrid(np.linspace(-4, 8, 100), np.linspace(-4, 8, 100))\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "rv1 = multivariate_normal(mean1, cov1)\n",
    "rv2 = multivariate_normal(mean2, cov2)\n",
    "rv3 = multivariate_normal(mean3, cov3)\n",
    "\n",
    "z1 = rv1.pdf(pos)\n",
    "z2 = rv2.pdf(pos)\n",
    "z3 = rv3.pdf(pos)\n",
    "\n",
    "# --- Subplots 2-4: Three 3D views ---\n",
    "views = [(30, 45), (0, 120), (20, 200)]\n",
    "titles = [\"3D View 1\", \"3D View 2\", \"3D View 3\"]\n",
    "\n",
    "for i, (view, title) in enumerate(zip(views, titles), start=2):\n",
    "    ax = fig.add_subplot(1, 4, i, projection=\"3d\")\n",
    "    ax.plot_surface(x, y, z1, cmap=\"Blues\", linewidth=0, alpha=0.8)\n",
    "    ax.plot_surface(x, y, z2, cmap=\"Reds\", linewidth=0, alpha=0.8)\n",
    "    ax.plot_surface(x, y, z3, cmap=\"Greens\", linewidth=0, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"X1\")\n",
    "    ax.set_ylabel(\"X2\")\n",
    "    ax.set_zlabel(\"Density\")\n",
    "    ax.view_init(elev=view[0], azim=view[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Train a Gaussian Mixture Model (GMM) using Expectation–Maximization (8 points)\n",
    "\n",
    "Now that you have generated three Gaussian distributions, let’s fit a **Gaussian Mixture Model (GMM)** to the data.  \n",
    "We will implement the **Expectation–Maximization (EM)** algorithm manually to show all steps:  \n",
    "\n",
    "1. **Initialization**  \n",
    "   - Randomly initialize the means, variances, and mixture weights (π).  \n",
    "\n",
    "2. **E-step**  \n",
    "   - Compute the \"responsibilities\", i.e., the probability that each point belongs to each Gaussian.  \n",
    "\n",
    "3. **M-step**  \n",
    "   - Update the means, covariances, and mixture weights using the responsibilities.  \n",
    "\n",
    "4. **Iterate** until convergence (or fixed number of steps).  \n",
    "\n",
    "This process maximizes the **likelihood** of the data under the mixture model.  \n",
    "\n",
    "Your tasks:  \n",
    "- Initialize Means, Covariances and weights.  \n",
    "- Implement the **E-step** and **M-step** functions.  \n",
    "- Train the model for ~20 iterations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Random Initialization (2 points)\n",
    "\n",
    "Before running the **Expectation–Maximization (EM)** algorithm, we need to **initialize the parameters** of our Gaussian Mixture Model (GMM).  \n",
    "Remember, GMM has three types of parameters:  \n",
    "\n",
    "- **Means (μ):** the center of each Gaussian cluster  \n",
    "- **Covariance matrices (Σ):** the spread/shape of each Gaussian  \n",
    "- **Weights (π):** the mixing proportions of each cluster  \n",
    "\n",
    "**Your task:**  \n",
    "1. Randomly pick **K points** from the dataset `X` as the initial means.  \n",
    "   - Hint: use `np.random.choice`.  \n",
    "2. Initialize each covariance matrix as the **overall covariance of the dataset**.  \n",
    "   - Hint: use `np.cov(X, rowvar=False)`.  \n",
    "3. Initialize the weights to be **equal** (e.g., `1/K` for each cluster).  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Stack all three distributions into one dataset\n",
    "X = np.vstack([data1, data2, data3])\n",
    "np.random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: Random Initialization \n",
    "# ----------------------------\n",
    "K = 3\n",
    "N, D = X.shape\n",
    "\n",
    "indices = np.random.choice(N, size=K, replace=False)\n",
    "means_init = X[indices].astype(float)\n",
    "\n",
    "overall_cov = np.cov(X, rowvar=False)\n",
    "covs_init = np.array([overall_cov.copy() for _ in range(K)])\n",
    "\n",
    "weights_init = np.full(K, 1.0 / K)\n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Initial Means:\\n\", means_init)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Expectation Step (E-step) (3 points)\n",
    "\n",
    "In the **E-step**, we compute the **responsibilities** — the probabilities that each data point belongs to each Gaussian cluster, given the current parameters.  \n",
    "\n",
    "Mathematically, for each point $x_i$ and cluster $k$:  \n",
    "\n",
    "$$\n",
    "r_{ik} = \\frac{\\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\pi_k$ = weight of cluster $k$  \n",
    "- $\\mu_k, \\Sigma_k$ = mean and covariance of cluster $k$  \n",
    "- $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ = Gaussian PDF  \n",
    "\n",
    "**Your task:**  \n",
    "1. For each cluster $k$, compute the Gaussian PDF values for all data points.  \n",
    "   - Hint: use `scipy.stats.multivariate_normal.pdf`.  \n",
    "2. Multiply each PDF by the corresponding cluster weight $\\pi_k$.  \n",
    "\n",
    "The output should be an **$N \\times K$ matrix** of responsibilities.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def e_step(X, means, covariances, weights):\n",
    "    N, D = X.shape\n",
    "    K = len(means)\n",
    "    resp = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        # ----------------------------\n",
    "        # TODO: Implement Expectation Step \n",
    "        # ----------------------------\n",
    "        rv = multivariate_normal(mean=means[k], cov=covariances[k])\n",
    "        resp[:, k] = weights[k] * rv.pdf(X)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Implementation Ends Here\n",
    "        # ----------------------------\n",
    "    resp /= resp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Maximization Step (M-step) (3 points)\n",
    "\n",
    "In the **M-step**, we update the parameters of our Gaussian Mixture Model (means, covariances, and weights) using the responsibilities $r_{ik}$ from the E-step.  \n",
    "\n",
    "#### Parameter updates:\n",
    "\n",
    "- **Effective number of points assigned to cluster $k$:**\n",
    "$$\n",
    "N_k = \\sum_{i=1}^N r_{ik}\n",
    "$$\n",
    "\n",
    "- **Updated means:**\n",
    "$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} x_i\n",
    "$$\n",
    "\n",
    "- **Updated covariance matrices:**\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "- **Updated weights:**\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "#### Your task:\n",
    "\n",
    "1. Use $N_k$ to update the **means**.  \n",
    "2. Update the **covariance matrices** by weighting squared deviations.  \n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def m_step(X, resp):\n",
    "\n",
    "    N, D = X.shape\n",
    "    K = resp.shape[1]\n",
    "    Nk = resp.sum(axis=0)  \n",
    "    means = np.dot(resp.T, X) / Nk[:, np.newaxis]\n",
    "    covariances = []\n",
    "    for k in range(K):\n",
    "        # ----------------------------\n",
    "        # TODO: Implement Maximization Step\n",
    "        # ----------------------------\n",
    "        x_centered = X - means[k]\n",
    "        w = resp[:, k][:, np.newaxis]\n",
    "        cov_k = (w * x_centered).T @ x_centered / Nk[k]\n",
    "        cov_k += 1e-6 * np.eye(D)\n",
    "        covariances.append(cov_k)\n",
    "        \n",
    "        # ----------------------------\n",
    "        # Implementation Ends Here\n",
    "        # ----------------------------\n",
    "    weights = Nk / N\n",
    "\n",
    "\n",
    "\n",
    "    return means, covariances, weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Train GMM with Expectation-Maximization (4 points)\n",
    "\n",
    "Now that we have implemented the **E-step** and **M-step**, we can put everything together into a training loop.\n",
    "\n",
    "The EM algorithm iteratively refines the parameters until convergence:\n",
    "\n",
    "1. **E-step:**  \n",
    "   Compute the responsibilities $r_{ik}$ given the current parameters.\n",
    "\n",
    "2. **M-step:**  \n",
    "   Update the means, covariances, and weights using the formulas from Step 3.\n",
    "\n",
    "3. **Repeat:**  \n",
    "   Run E-step and M-step in a loop until convergence (or for a fixed number of iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Your task:\n",
    "- Complete the training loop for **n_iters** iterations:\n",
    "  - Call `e_step` to compute responsibilities.  \n",
    "  - Call `m_step` to update the parameters.  \n",
    "\n",
    "At the end, the function should return the **final means, covariances, and weights** of the trained GMM.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gmm(X, means, covariances, weights, n_iters=20):\n",
    "    \"\"\"\n",
    "    Train a Gaussian Mixture Model using EM.\n",
    "    \n",
    "    Parameters:\n",
    "        X : (n_samples, n_features) dataset\n",
    "        means, covariances, weights : initialized parameters\n",
    "        n_iters : number of iterations\n",
    "\n",
    "    Returns:\n",
    "        means, covariances, weights\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # TODO: Implement GMM Training (EM algorithm)\n",
    "    # ----------------------------\n",
    "    for _ in range(n_iters):\n",
    "        resp = e_step(X, means, covariances, weights)\n",
    "        means, covariances, weights = m_step(X, resp)\n",
    "   \n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "    return means, covariances, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Visualization function\n",
    "# ----------------------------\n",
    "def visualize_training(X, K=3, n_iters=20):\n",
    "    np.random.seed(42)\n",
    "    means = X[np.random.choice(len(X), K, replace=False)]\n",
    "    covariances = [np.cov(X, rowvar=False)] * K\n",
    "    weights = np.ones(K) / K\n",
    "    \n",
    "    colors = [\"red\", \"blue\", \"green\"]  \n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(22, 16))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        resp = e_step(X, means, covariances, weights)\n",
    "        means, covariances, weights = m_step(X, resp)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.scatter(X[:,0], X[:,1], alpha=0.08, color=\"gray\")\n",
    "        \n",
    "\n",
    "        for k, color in enumerate(colors[:K]):\n",
    "            ax.scatter(means[k,0], means[k,1], c=color, marker=\"x\", s=70, linewidth=2)\n",
    "            \n",
    "            mu_str = np.round(means[k], 2)\n",
    "            cov_diag = np.round(np.diag(covariances[k]), 2)\n",
    "            ax.text(\n",
    "                0.02, 0.95 - 0.08*k,   \n",
    "                f\"μ{k+1}={mu_str.tolist()}, Σ diag={cov_diag.tolist()}\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8, color=color, ha=\"left\", va=\"top\", family=\"monospace\"\n",
    "            )\n",
    "        \n",
    "        ax.set_title(f\"Iteration {i+1}\", fontsize=12, fontweight=\"bold\", pad=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run visualization\n",
    "# ----------------------------\n",
    "final_means, final_covs, final_weights = train_gmm(X, means_init, covs_init, weights_init, n_iters=20)\n",
    "visualize_training(X, K=3, n_iters=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Behavior of Gaussian Mixture Models (2 points)\n",
    "\n",
    "In this exercise, instead of implementing EM from scratch, we will use **Scikit-learn’s `GaussianMixture`** class.\n",
    "\n",
    "### How to use the GMM Library\n",
    "1. Import and create a model:\n",
    "   ```python\n",
    "   from sklearn.mixture import GaussianMixture\n",
    "   gmm = GaussianMixture(n_components=2, covariance_type=\"full\", random_state=42)\n",
    "\n",
    "### Means and Covariances Used\n",
    "\n",
    "We test GMM under different scenarios by **changing the means and covariances** of two Gaussians:\n",
    "\n",
    "- **Scenario 1:** Well-separated clusters with equal variances *(easy case)*.  \n",
    "- **Scenario 2:** Overlapping clusters where one has larger variance.  \n",
    "- **Scenario 3:** Both Gaussians have the same mean but very different variances.  \n",
    "\n",
    "These examples illustrate how GMM fits data under varying conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Limitations of GMM on 1D Gaussians\n",
    "# ----------------------------\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "scenarios = [\n",
    "    (0, 1, 5, 1),    # Case 1: equal variance, well separated\n",
    "    (0, 1, 2, 4),    # Case 2: overlap + different spread\n",
    "    (0, 0.5, 0, 5)   # Case 3: same mean, very different variances\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(scenarios), 2, figsize=(14, 4*len(scenarios)))\n",
    "\n",
    "for i, (m1, v1, m2, v2) in enumerate(scenarios):\n",
    "    # Generate data\n",
    "    data1 = np.random.normal(loc=m1, scale=np.sqrt(v1), size=1000)\n",
    "    data2 = np.random.normal(loc=m2, scale=np.sqrt(v2), size=1000)\n",
    "    X_ex = np.concatenate([data1, data2])[:, None]  \n",
    "    df_ex = pd.DataFrame({\n",
    "        \"value\": np.concatenate([data1, data2]),\n",
    "        \"distribution\": [\"Gaussian 1\"]*len(data1) + [\"Gaussian 2\"]*len(data2)\n",
    "    })\n",
    "    \n",
    "    # Plot ground truth PDFs\n",
    "    x_range = np.linspace(X_ex.min()-2, X_ex.max()+2, 1000)\n",
    "    pdf1 = norm.pdf(x_range, loc=m1, scale=np.sqrt(v1))\n",
    "    pdf2 = norm.pdf(x_range, loc=m2, scale=np.sqrt(v2))\n",
    "    \n",
    "    axes[i,0].plot(x_range, pdf1, 'b-', linewidth=2, label='Gaussian 1', alpha=0.8)\n",
    "    axes[i,0].plot(x_range, pdf2, 'r-', linewidth=2, label='Gaussian 2', alpha=0.8)\n",
    "    axes[i,0].fill_between(x_range, pdf1, alpha=0.3, color='blue')\n",
    "    axes[i,0].fill_between(x_range, pdf2, alpha=0.3, color='red')\n",
    "    \n",
    "    # Add data points as scatter plot\n",
    "    axes[i,0].scatter(data1, np.zeros_like(data1) + 0.1, alpha=0.3, s=1, color='blue')\n",
    "    axes[i,0].scatter(data2, np.zeros_like(data2) + 0.1, alpha=0.3, s=1, color='red')\n",
    "    \n",
    "    axes[i,0].set_title(f\"Scenario {i+1}: Ground Truth\")\n",
    "    axes[i,0].set_xlabel(\"X\")\n",
    "    axes[i,0].set_ylabel(\"Density\")\n",
    "    axes[i,0].legend()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # TODO: SKlearn GMM training \n",
    "    # ----------------------------\n",
    "    gmm = GaussianMixture(n_components=2, covariance_type=\"full\", random_state=42)\n",
    "    gmm.fit(X_ex)\n",
    "    labels = gmm.predict(X_ex)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Implementation ends here\n",
    "    # ----------------------------\n",
    "\n",
    "    # Plot GMM learned PDFs\n",
    "    # Get learned parameters\n",
    "    means_learned = gmm.means_.flatten()\n",
    "    covariances_learned = gmm.covariances_.flatten()\n",
    "    weights_learned = gmm.weights_\n",
    "    \n",
    "    # Plot individual component PDFs\n",
    "    pdf1_learned = norm.pdf(x_range, loc=means_learned[0], scale=np.sqrt(covariances_learned[0]))\n",
    "    pdf2_learned = norm.pdf(x_range, loc=means_learned[1], scale=np.sqrt(covariances_learned[1]))\n",
    "    \n",
    "    # Weighted mixture PDF\n",
    "    mixture_pdf = weights_learned[0] * pdf1_learned + weights_learned[1] * pdf2_learned\n",
    "    \n",
    "    axes[i,1].plot(x_range, pdf1_learned, 'b-', linewidth=2, label=f'Component 1 (μ={means_learned[0]:.2f})', alpha=0.8)\n",
    "    axes[i,1].plot(x_range, pdf2_learned, 'r-', linewidth=2, label=f'Component 2 (μ={means_learned[1]:.2f})', alpha=0.8)\n",
    "    axes[i,1].plot(x_range, mixture_pdf, 'k--', linewidth=2, label='Mixture PDF', alpha=0.8)\n",
    "    axes[i,1].fill_between(x_range, pdf1_learned, alpha=0.3, color='blue')\n",
    "    axes[i,1].fill_between(x_range, pdf2_learned, alpha=0.3, color='red')\n",
    "    \n",
    "    # Add data points colored by cluster assignment\n",
    "    cluster1_data = X_ex[labels == 0].flatten()\n",
    "    cluster2_data = X_ex[labels == 1].flatten()\n",
    "    \n",
    "    axes[i,1].scatter(cluster1_data, np.zeros_like(cluster1_data) + 0.1, alpha=0.3, s=1, color='blue')\n",
    "    axes[i,1].scatter(cluster2_data, np.zeros_like(cluster2_data) + 0.1, alpha=0.3, s=1, color='red')\n",
    "    \n",
    "    axes[i,1].set_title(f\"Scenario {i+1}: GMM Fit\")\n",
    "    axes[i,1].set_xlabel(\"X\")\n",
    "    axes[i,1].set_ylabel(\"Density\")\n",
    "    axes[i,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (6 points)\n",
    "\n",
    "1. **Model Parameters:**  \n",
    "   - What are the **final means and covariance matrices** that the model converged to?  \n",
    "   - Do they align well with the Gaussian clusters you originally generated?  \n",
    "\n",
    "   **Answer:**  \n",
    "The final means near [3.96, 4.07], [0.01, 4.98], and [−0.02, −0.01] correspond closely to the true cluster centers you generated at [4, 4], [0, 5], and [0, 0]. The covariance matrices also show similar shapes and orientations to the original ones, indicating that the EM algorithm correctly captured both the positions and spreads of the three Gaussian components.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Longer Training (40 steps):**  \n",
    "   - What do you think would happen if we ran the EM algorithm for **40 steps** instead of 20?  \n",
    "\n",
    "   **Answer:**  \n",
    "This would not change the results significantly as the parameters have stabilized by iteration 20. Additional iterations would only cause negligible improvements. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "3. **Early Stopping (10 steps):**  \n",
    "   - What would have happened if we had stopped after **10 steps**? Would the model have converged?  \n",
    "\n",
    "   **Answer:**  \n",
    "At iteration 10, the means were approximately μ₁ = [3.69, 4.19], μ₂ = [−0.13, 4.93], and μ₃ = [−0.02, 0.0], which are close but not yet fully aligned with the true cluster centers. The covariance values were also still adjusting (e.g., Σ₁ diag = [1.91, 1.3]), showing that the clusters were roughly identified but not yet stable. This indicates the model was partially converged but required additional iterations to fine-tune parameters and fully separate the distributions.\n",
    "\n",
    "---\n",
    "4. **GMM with same means with different variance:**  \n",
    "   - In Scenario 3 of the Exercise 5, both the gaussians have same means but different variance. Why can GMM separate two components even though they share the same mean?\n",
    "\n",
    "   **Answer:**  \n",
    "In Scenario 3, even though both Gaussians share the same mean, the GMM can still distinguish them because it models not only the mean but also the variance and weight of each component. Points near the center are more likely assigned to the tighter (smaller variance) Gaussian, while those farther away are assigned to the broader (larger variance) Gaussian—allowing the model to separate them based on spread rather than location.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
